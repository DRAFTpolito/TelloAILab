{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bf8d4b-202b-43fe-8f54-06c76171b974",
   "metadata": {
    "id": "97bf8d4b-202b-43fe-8f54-06c76171b974"
   },
   "source": [
    "# Tello for AI-Hackers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e4d24e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "Now, we are going to move Tello with out hands!!\n",
    "\n",
    "We'll use a [Neural Network (NN)](https://en.wikipedia.org/wiki/Neural_network) to predict what we want the drone to do; whether it should move forward, backward or stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81e7d0",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks\n",
    "An Artificial Neural Network (ANN) models the connections of the biological neurons as weights between nodes. An ANN is usually composed by multiple layers of nodes, because this lets us insert multiple non-linearities thus allowing a hierarchical decomposition of the input, possibly reducing the number of parameters necessary to learn a certain task.\n",
    "\n",
    "<img src=\"resources/nn.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a97fd",
   "metadata": {},
   "source": [
    "Each node computes a linear combination of its inputs and the output then passes through an activation function, used to insert non-linearities.\n",
    "\n",
    "<img src=\"resources/nn_function.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46bac87",
   "metadata": {},
   "source": [
    "#### Our model: [ResNet-34](https://pytorch.org/hub/pytorch_vision_resnet/)\n",
    "In the years different type of ANN have been created and most of them are available in deep learning libraries, such as [PyTorch](https://pytorch.org/) or [Keras](https://keras.io/).\n",
    "\n",
    "We have decided to use a pre-trained version of ResNet-34 available in PyTorch, this is a model taken from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) and trained on [ImageNet](https://www.image-net.org/), a dataset used for an annual competition between 2010 and 2017. \n",
    "\n",
    "Given that the model is pre-trained on a huge amount of images, we can assume that it has already learned a lot of useful notions on how object in images are represented and we may hope that these features will in same way be similar to those of our few images.\n",
    "\n",
    "Otherwise, it would be extremely difficult to train such a big network with a few pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85feb246",
   "metadata": {},
   "source": [
    "### Dataset introduction\n",
    "Our task falls into the category of [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning), because we want the NN to learn a function which maps 3 hands gestures (classes) to 3 commands using a set of example (image)-label (class) pairs, as shown in [Visualize some of the images in training](#visualize).\n",
    "\n",
    "- **Fist** -> move forward\n",
    "- **One Open Hand** -> move backward\n",
    "- **Two Open Hands** -> stop\n",
    "\n",
    "Actually, we're also going to create a 4th class, named \"other\", which will cover all the other cases and that is going to be associated to the \"stop\" command.\n",
    "\n",
    "This set of example-label pairs will be our [dataset](#dataset), we'll make the model predict a label for each example (image) and then we're going to modifiy its parameters, i.e. the weights between nodes, based on a [loss function](https://en.wikipedia.org/wiki/Loss_function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8fa8f",
   "metadata": {},
   "source": [
    "### Loss Function & Metric\n",
    "We want to maximise the accuracy, our metric, and in order to do so we have chosen to minimise the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "\n",
    "\n",
    "We'll use this loss function instead of the accuracy to update the parameters of the model, because, in contrast with out metric, it is continous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb192f",
   "metadata": {},
   "source": [
    "### Training\n",
    "To [train](#train) the model one has to create a training loop with the following steps:\n",
    "1) sample a minibatch from the dataset, where a minibatch is a set of example-label pairs\n",
    "\n",
    "2) perform inference with the model\n",
    "\n",
    "3) computing the loss between predicted and true labels\n",
    "\n",
    "4) computing the gradient\n",
    "\n",
    "5) using the gradient to update the model's parameters\n",
    "\n",
    "6) go back to 1) until the end of the dataset\n",
    "\n",
    "7) with gradient disabled, perform inference on all the validation dataset\n",
    "\n",
    "8) if the loss on the validation dataset plateaus or starts to diverge, end traing\n",
    "\n",
    "TIPS:\n",
    "- Usually the minibatch size is chosen to be a power of 2, for performances reason on a GPU.\n",
    "- Evaluating the performance of the model on a validation dataset is needed to reduce the effect of overfitting, i.e. learning regularities that are present only in the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85499550",
   "metadata": {},
   "source": [
    "### Have Fun!\n",
    "It's time to use the trained model to control our Tello.\n",
    "\n",
    "Complete the code below and ask us to test it ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a4ae1",
   "metadata": {
    "id": "770a4ae1"
   },
   "source": [
    "## Packages imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a7b70",
   "metadata": {
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1647808017863,
     "user": {
      "displayName": "Carlo Cena",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggfgc5L4HtHz06kz-WjXdFuebvUkEdgtYJrVZE0EQ=s64",
      "userId": "14201859508342393884"
     },
     "user_tz": 0
    },
    "id": "e15a7b70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "\n",
    "# custom libraries\n",
    "from pkgs.telloCV import TelloCV\n",
    "\n",
    "from sys import platform\n",
    "if platform == \"win32\":\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a7531",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b1dd5-65b6-489e-90de-b461e115285e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71735e-74dc-4f5b-95c1-7fde6fb745b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "MODEL_PATH = '../models/best_model.th'\n",
    "CLASS_NAMES = [\"forward\", \"backward\", \"stop\", \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458519d-d8cb-49dc-b4da-8276cb839488",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Data augmentation for training\n",
    "# Just resize and gray scale for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=.2, hue=.1),\n",
    "        transforms.RandomAffine(degrees=(0, 10), translate=(0.2, 0.2), scale=(0.75, 1.0)),\n",
    "        transforms.GaussianBlur((3, 3), sigma=(1.5, 2.5)),\n",
    "        transforms.Grayscale(1),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.Grayscale(1),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_PATH, x), \n",
    "                                          data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=BATCH_SIZE, \n",
    "                                              shuffle=True, \n",
    "                                              num_workers=2) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519625f-6120-4db3-935f-864cb3ed59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it is possible to use the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01907",
   "metadata": {},
   "source": [
    "<a id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3beb1-e4c6-4ef0-845a-8b290cbef18e",
   "metadata": {},
   "source": [
    "## Visualize some of the images in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912f369-e5d1-46b0-8187-b2aedc49aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1).squeeze()\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(8)\n",
    "    f.set_figheight(6)\n",
    "    plt.imshow(inp, cmap='gray')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "inputs = inputs[:5]\n",
    "classes = classes[:5]\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1731f",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398321c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline Model\n",
    "The trainer class will read the dataset, create the dataloaders and the model. If 'saved_model' is used, the given weights will be loaded.\n",
    "\n",
    "[Convolutional layers](https://en.wikipedia.org/wiki/Convolutional_neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f61e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "# freeze all the parameters of the model\n",
    "for i, param in enumerate(model.parameters()):\n",
    "        param.requires_grad = False\n",
    "    \n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(num_ftrs, len(CLASS_NAMES))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a92afe-d67d-4803-92a0-6ab1d92babd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020665f",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1b566-44db-4693-8405-c66ab8fe6433",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b1c6a-270f-4393-8add-207d2588879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders,  criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17448f46-c156-4ee7-85e0-c7ffc79d3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f0270-f2dd-466d-9806-2ff3f2d6afa9",
   "metadata": {},
   "source": [
    "### Run offline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1f329-ef4e-4c27-9f7f-e04394222e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloaders, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).cpu()\n",
    "            outputs = nn.functional.softmax(outputs, dim=-1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1a60c-47a1-4e84-9a8b-fec6d9b26291",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca966d",
   "metadata": {},
   "source": [
    "### Test live inference\n",
    "Lets test the model from the computer's camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09701e45-3ccc-4427-ba0c-d0490fd12d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68617df-bc73-4660-b92f-7dcbd7d7b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    x = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.medianBlur(x, 9)  # Reduce impulse noise\n",
    "    x = cv2.GaussianBlur(x, (3, 3), 3.0)  # Reduce linear noise\n",
    "    x = x/255.0\n",
    "    x = x[None, None, ...]  # Adding batch and channel dimensions\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = x.to(device)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d8d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"test_model\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('test_model', 800, 600)\n",
    "cont = 0\n",
    "scores = np.zeros(len(CLASS_NAMES))\n",
    "probs = np.zeros(len(CLASS_NAMES))\n",
    "stats = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    cont += 1\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    frame2 = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    process_frame = preprocess(frame2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(process_frame).cpu()\n",
    "        output = nn.functional.softmax(output, dim=-1)\n",
    "        index_pred = np.argmax(output)\n",
    "        scores[index_pred] += 1\n",
    "        probs += output.detach().numpy()[0]\n",
    "            \n",
    "    if cont >= 5:  # Voting across 5 frames\n",
    "        index_pred = np.argmax(scores)\n",
    "        pred_str = CLASS_NAMES[index_pred]\n",
    "        stats = [\"\", \"Prediction: \" + str(pred_str)]\n",
    "        stats.append(\"Output: \" + str(probs/5))\n",
    "        cont = 0\n",
    "        frame_window = []\n",
    "        scores = np.zeros(len(CLASS_NAMES))\n",
    "        probs = np.zeros(len(CLASS_NAMES))\n",
    "        \n",
    "    for idx, text in enumerate(stats):\n",
    "        cv2.putText(frame, text, (0, 30 + (idx * 30)), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, (0, 0, 255), lineType=30)\n",
    "\n",
    "    cv2.imshow(\"test_model\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61c766",
   "metadata": {},
   "source": [
    "## Inference and Command\n",
    "\n",
    "Receiving images from Tello, performing inference with the model and sending a command back to Tello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb37257-d404-41f1-94d9-a1fb3b392ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5517f31-228e-42ca-a668-58f18868e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize drone\n",
    "tellotrack = TelloCV()\n",
    "tellotrack.init_drone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"main_loop\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('main_loop', 800, 600)\n",
    "cont = 0\n",
    "scores = np.zeros(len(CLASS_NAMES))\n",
    "probs = np.zeros(len(CLASS_NAMES))\n",
    "stats = []\n",
    "\n",
    "tellotrack.drone.takeoff()\n",
    "try:\n",
    "    # skip first 300 frames\n",
    "    frame_skip = 300\n",
    "    while True:\n",
    "        for frame in tellotrack.container.decode(video=0):\n",
    "            if 0 < frame_skip:\n",
    "                frame_skip = frame_skip - 1\n",
    "                continue\n",
    "            start_time = time.time()\n",
    "            img, frame = tellotrack.process_frame(frame)\n",
    "            frame2 = np.array(frame.to_image())\n",
    "            img = preprocess(img)\n",
    "            cont += 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                print(img.shape)\n",
    "                output = model(img).cpu()\n",
    "                output = nn.functional.softmax(output, dim=-1)\n",
    "                index_pred = np.argmax(output)\n",
    "                scores[index_pred] += 1\n",
    "                probs += output.detach().numpy()[0]\n",
    "                \n",
    "            if cont >= 5:  # Voting across 5 frames\n",
    "                index_pred = np.argmax(scores)\n",
    "                pred_str = CLASS_NAMES[index_pred]\n",
    "                stats = [\"\", \"Prediction: \" + str(pred_str)]\n",
    "                stats.append(\"Output: \" + str(probs/5))\n",
    "                cont = 0\n",
    "                frame_window = []\n",
    "                scores = np.zeros(len(CLASS_NAMES))\n",
    "                probs = np.zeros(len(CLASS_NAMES))\n",
    "                \n",
    "                tellotrack.send_cmd(pred_str)\n",
    "\n",
    "            for idx, text in enumerate(stats):\n",
    "                cv2.putText(frame2, text, (0, 30 + (idx * 30)), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.5, (0, 0, 255), lineType=30)\n",
    "            cv2.imshow(\"main_loop\", frame2)\n",
    "            \n",
    "            if frame.time_base < 1.0/60:\n",
    "                time_base = 1.0/60\n",
    "            else:\n",
    "                time_base = frame.time_base\n",
    "                \n",
    "            frame_skip = int((time.time() - start_time)/time_base)\n",
    "            \n",
    "            k = cv2.waitKey(1)\n",
    "            if k%256 == 27:\n",
    "                # ESC pressed\n",
    "                print(\"Escape hit, closing...\")\n",
    "                break\n",
    "            \n",
    "        k = cv2.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "except Exception as ex:\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    traceback.print_exception(exc_type, exc_value, exc_traceback)\n",
    "    print(ex)\n",
    "finally:\n",
    "    tellotrack.drone.quit()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tellotrack.drone.land()\n",
    "tellotrack.drone.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2b689",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "As you have seen, this approach hasn't led to great performances during inference and it would be very risky to try to control a drone with the model. Therefore, we have decided to train an [object detection](https://en.wikipedia.org/wiki/Object_detection) model.\n",
    "\n",
    "In object detection the task is to both classify and localize an object. There are different ways to do so, our model, [MobileNet-v2](https://paperswithcode.com/lib/torchvision/mobilenet-v2) taken from [Tenforflow 2 Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html), predicts a set of 6 numbers for each anchor, a predefined bounding box in the image.\n",
    "\n",
    "The 6 numbers are composed as follows:\n",
    "- 4 for bounding box translation and scale\n",
    "- 1 for confidence score\n",
    "- 1 for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ea00e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sys import platform\n",
    "# Clone the tensorflow models repository\n",
    "if \"models_tf2_api\" in pathlib.Path.cwd().parts:\n",
    "    while \"models_tf2_api\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models_tf2_api').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models \"models_tf2_api\"\n",
    "\n",
    "# Install Object Detection API\n",
    "if platform == \"win32\":\n",
    "    if \"coco_api\" in pathlib.Path.cwd().parts:\n",
    "        while \"coco_api\" in pathlib.Path.cwd().parts:\n",
    "            os.chdir('..')\n",
    "    elif not pathlib.Path('coco_api').exists():\n",
    "        !git clone --depth 1 https://github.com/cocodataset/cocoapi \"coco_api\"\n",
    "    !(cd coco_api/PythonAPI/ && echo F|xcopy /S /Q /Y /F \"../../resources/setup_coco.py\" setup.py && python setup.py build_ext --inplace)\n",
    "    \n",
    "    !(cd models_tf2_api/research/ && protoc object_detection/protos/*.proto --python_out=. && echo F|xcopy /S /Q /Y /F \"../../resources/setup_od.py\" setup.py && python -m pip install . --user)\n",
    "else:\n",
    "    !(cd models_tf2_api/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d45df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_detection_function(model):\n",
    "    \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "        image, shapes = model.preprocess(image)\n",
    "        prediction_dict = model.predict(image, shapes)\n",
    "        detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "        return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "    return detect_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f8646",
   "metadata": {},
   "source": [
    "### Download model & load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a74870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "model_dir = '../models/graph_model/checkpoint/ckpt-0'\n",
    "pipeline_config = '../models/graph_model/pipeline.config'\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(\n",
    "      model=detection_model)\n",
    "ckpt.restore(os.path.join(model_dir))\n",
    "\n",
    "# get model function\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a4d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "\n",
    "# map labels for inference decoding\n",
    "label_map_path = configs['eval_input_config'].label_map_path\n",
    "label_map = label_map_util.load_labelmap(label_map_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map,\n",
    "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
    "    use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"test_obj_det\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('test_obj_det', 800, 600)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(img, 0), dtype=tf.float32)\n",
    "    input_tensor = tf.image.resize(input_tensor, (320, 320))\n",
    "    detections, _, _ = detect_fn(input_tensor)\n",
    "    scores = detections[\"detection_scores\"][0].numpy()\n",
    "    classes = detections[\"detection_classes\"][0].numpy()\n",
    "    bboxes = detections[\"detection_boxes\"][0].numpy()\n",
    "    index_scores = np.where(scores>0.40)[0]  # 0.40 threshold\n",
    "\n",
    "    if len(index_scores) > 0:\n",
    "        pred_str = \"other\"\n",
    "        for id_score in index_scores:\n",
    "            index_pred = int(classes[id_score])\n",
    "            if not CLASS_NAMES[index_pred] == \"other\":  # not face found\n",
    "                pred_str = CLASS_NAMES[index_pred]\n",
    "                break                                   # Stopping at first valid command\n",
    "    else:\n",
    "        pred_str = \"other\"\n",
    "            \n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = frame.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=3,\n",
    "          min_score_thresh=.40,\n",
    "          agnostic_mode=False,\n",
    "    )\n",
    "    cv2.imshow(\"test_obj_det\", image_np_with_detections)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df11c",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be294ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize drone\n",
    "tellotrack = TelloCV()\n",
    "tellotrack.init_drone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"main_loop_obj_det\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('main_loop_obj_det', 800, 600)\n",
    "\n",
    "tellotrack.drone.takeoff()\n",
    "try:\n",
    "    # skip first 300 frames\n",
    "    frame_skip = 300\n",
    "    while True:\n",
    "        for frame in tellotrack.container.decode(video=0):\n",
    "            if 0 < frame_skip:\n",
    "                frame_skip = frame_skip - 1\n",
    "                continue\n",
    "            start_time = time.time()\n",
    "            img, frame = tellotrack.process_frame(frame)\n",
    "            # frame = np.array(frame.to_image())\n",
    "            \n",
    "            input_tensor = tf.convert_to_tensor(\n",
    "                np.expand_dims(img, 0), dtype=tf.float32)\n",
    "            input_tensor = tf.image.resize(input_tensor, (320, 320))\n",
    "            detections, _, _ = detect_fn(input_tensor)\n",
    "            scores = detections[\"detection_scores\"][0].numpy()\n",
    "            classes = detections[\"detection_classes\"][0].numpy()\n",
    "            bboxes = detections[\"detection_boxes\"][0].numpy()\n",
    "            index_scores = np.where(scores>0.40)[0]  # 0.40 threshold\n",
    "\n",
    "            if len(index_scores) > 0:\n",
    "                pred_str = \"other\"\n",
    "                for id_score in index_scores:\n",
    "                    index_pred = int(classes[id_score])\n",
    "                    if not CLASS_NAMES[index_pred] == \"other\":  # not face found\n",
    "                        pred_str = CLASS_NAMES[index_pred]\n",
    "                        break                                   # Stopping at first valid command\n",
    "            else:\n",
    "                pred_str = \"other\"\n",
    "                \n",
    "            tellotrack.send_cmd(pred_str)  # Sending command to drone\n",
    "            \n",
    "            label_id_offset = 1\n",
    "            image_np_with_detections = img.copy()\n",
    "            viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np_with_detections,\n",
    "                  detections['detection_boxes'][0].numpy(),\n",
    "                  (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "                  detections['detection_scores'][0].numpy(),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  max_boxes_to_draw=3,\n",
    "                  min_score_thresh=.40,\n",
    "                  agnostic_mode=False,\n",
    "            )\n",
    "            cv2.imshow(\"main_loop_obj_det\", image_np_with_detections)\n",
    "            \n",
    "            if frame.time_base < 1.0/60:\n",
    "                time_base = 1.0/60\n",
    "            else:\n",
    "                time_base = frame.time_base\n",
    "                \n",
    "            frame_skip = int((time.time() - start_time)/time_base)\n",
    "            \n",
    "            k = cv2.waitKey(1)\n",
    "            if k%256 == 27:\n",
    "                # ESC pressed\n",
    "                print(\"Escape hit, closing...\")\n",
    "                break\n",
    "            \n",
    "        k = cv2.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "except Exception as ex:\n",
    "    exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "    traceback.print_exception(exc_type, exc_value, exc_traceback)\n",
    "    print(ex)\n",
    "finally:\n",
    "    tellotrack.drone.quit()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tellotrack.drone.land()\n",
    "tellotrack.drone.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783ee8a",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "You may try to improve over this by increasing the amount of data, trying different architectures and/or performing more preprocessing, such as removing the background."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3.tello_for_AIhackers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
